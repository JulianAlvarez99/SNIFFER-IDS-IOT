{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 1 (Corregida): Carga, Descarga y An√°lisis (EDA)\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Importaci√≥n de Librer√≠as ---\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"Librer√≠as importadas correctamente.\")\n",
    "\n",
    "# Configuraci√≥n de Pandas y Matplotlib\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# --- 2. Descarga del Dataset (Usando Cach√©) ---\n",
    "print(\"\\nLocalizando el dataset (usar√° la cach√© si ya existe)...\")\n",
    "download_path = \"\"\n",
    "try:\n",
    "    # -----------------------------------------------------------------\n",
    "    # NOTA: Esta funci√≥n es 'cache-aware'.\n",
    "    # Si ya descargaste el dataset, NO lo volver√° a bajar.\n",
    "    # Simplemente te dar√° la ruta a los archivos existentes en cach√©.\n",
    "    # -----------------------------------------------------------------\n",
    "    download_path = kagglehub.dataset_download(\"madhavmalhotra/unb-cic-iot-dataset\")\n",
    "    print(f\"Dataset (desde cach√© o descarga) localizado en: {download_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al descargar: {e}\")\n",
    "    print(\"Por favor, verifica tu conexi√≥n y que 'kaggle.json' est√© en el lugar correcto.\")\n",
    "\n",
    "\n",
    "# --- 3. B√∫squeda y Carga del CSV ---\n",
    "# Esta es la ruta base que devuelve kagglehub\n",
    "base_download_path = download_path\n",
    "\n",
    "# Construimos la ruta correcta que descubrimos\n",
    "csv_directory = os.path.join(base_download_path, 'wataiData', 'csv', 'CICIoT2023')\n",
    "print(f\"\\nAccediendo al directorio de CSVs: {csv_directory}\")\n",
    "\n",
    "csv_file_path = \"\"\n",
    "df_sample = None\n",
    "\n",
    "try:\n",
    "    # Listar todos los archivos en ese directorio\n",
    "    all_csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "    if not all_csv_files:\n",
    "        print(\"ERROR: No se encontraron archivos .csv en el directorio especificado.\")\n",
    "    else:\n",
    "        print(f\"¬°√âxito! Se encontraron {len(all_csv_files)} archivos CSV.\")\n",
    "\n",
    "        # --- 4. Carga de MUESTRA (Sampling) ---\n",
    "        # No cargaremos los 169 archivos.\n",
    "        # Para el an√°lisis, cargaremos solo el PRIMER archivo de la lista.\n",
    "\n",
    "        first_csv_file = all_csv_files[0]\n",
    "        csv_file_path = os.path.join(csv_directory, first_csv_file)\n",
    "\n",
    "        print(f\"\\nCargando el primer archivo como muestra: {first_csv_file}...\")\n",
    "\n",
    "        df_sample = pd.read_csv(csv_file_path)\n",
    "\n",
    "        print(\"Muestra cargada correctamente.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: El directorio {csv_directory} no existe. Verifica la ruta.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al listar o cargar archivos: {e}\")\n",
    "\n",
    "\n",
    "# --- 5. An√°lisis de Clases (Si la carga fue exitosa) ---\n",
    "if df_sample is not None:\n",
    "    try:\n",
    "        print(\"\\n--- Informaci√≥n General del Dataset (Muestra del primer CSV) ---\")\n",
    "        df_sample.info()\n",
    "\n",
    "        print(\"\\n--- Primeras 5 filas (para ver columnas) ---\")\n",
    "        print(df_sample.head())\n",
    "\n",
    "        #################################################################\n",
    "        # ¬°IMPORTANTE!\n",
    "        # Revisa la salida de 'df_sample.head()' y confirma\n",
    "        # el nombre de la columna de etiquetas.\n",
    "        # C√°mbialo aqu√≠ si es necesario.\n",
    "        LABEL_COLUMN_NAME = 'label'\n",
    "        #################################################################\n",
    "\n",
    "        if LABEL_COLUMN_NAME in df_sample.columns:\n",
    "            print(f\"\\n--- Distribuci√≥n de Clases (Columna: '{LABEL_COLUMN_NAME}') ---\")\n",
    "            class_distribution = df_sample[LABEL_COLUMN_NAME].value_counts()\n",
    "            print(class_distribution)\n",
    "\n",
    "            # --- 6. Visualizaci√≥n de Clases ---\n",
    "            print(\"\\nGenerando gr√°fico de distribuci√≥n de clases...\")\n",
    "            plt.figure(figsize=(12, 10))\n",
    "\n",
    "            sns.countplot(y=df_sample[LABEL_COLUMN_NAME],\n",
    "                          order=class_distribution.index)\n",
    "\n",
    "            plt.title(f'Distribuci√≥n de Clases (Archivo: {first_csv_file})')\n",
    "            plt.xlabel('Cantidad de Muestras (Escala Logar√≠tmica)')\n",
    "            plt.ylabel('Tipo de Tr√°fico (Ataque / Benigno)')\n",
    "            plt.xscale('log')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n--- ERROR DE AN√ÅLISIS ---\")\n",
    "            print(f\"No se encontr√≥ la columna '{LABEL_COLUMN_NAME}'.\")\n",
    "            print(f\"Las columnas disponibles son: {df_sample.columns.tolist()}\")\n",
    "            print(\"Por favor, actualiza la variable 'LABEL_COLUMN_NAME' en el script.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error durante el an√°lisis: {e}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 2 (Corregida): Preprocesamiento y Limpieza\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Importaciones Adicionales ---\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# (Asumimos que 'df_sample' todav√≠a existe en memoria de la Etapa 1)\n",
    "# Si no, vuelve a cargar el primer CSV:\n",
    "# csv_file_path = ... (ruta al primer csv)\n",
    "# df_sample = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(f\"--- DataFrame Original (Muestra) ---\")\n",
    "print(f\"Forma original: {df_sample.shape}\")\n",
    "\n",
    "# --- 2. üßπ Limpieza de Nombres de Columnas ---\n",
    "# ¬°IMPORTANTE! Arreglamos los nombres con espacios (ej. 'Protocol Type')\n",
    "# Reemplazamos ' ' por '_' y convertimos a min√∫sculas\n",
    "original_columns = df_sample.columns.tolist()\n",
    "df_sample.columns = [col.strip().replace(' ', '_').lower() for col in df_sample.columns]\n",
    "new_columns = df_sample.columns.tolist()\n",
    "\n",
    "print(\"\\n--- Nombres de Columnas Corregidos ---\")\n",
    "print(\"Originales:\", original_columns)\n",
    "print(\"Nuevos:\", new_columns)\n",
    "\n",
    "\n",
    "# --- 3. üßπ Limpieza de Datos (NaN e Infinitos) ---\n",
    "# Aunque .info() no mostr√≥ NaNs, los valores Infinitos (Inf)\n",
    "# pueden existir y no se reportan como NaN.\n",
    "# Reemplazamos Inf y -Inf con NaN\n",
    "df_processed = df_sample.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Ahora, si ese reemplazo cre√≥ NaNs, los llenamos con 0.\n",
    "# (Llenar con 0 es seguro en datos de red)\n",
    "df_processed = df_processed.fillna(0)\n",
    "print(\"\\n--- Limpieza de Infinitos y NaNs completada ---\")\n",
    "\n",
    "\n",
    "# --- 4. üìä Separaci√≥n de Caracter√≠sticas (X) y Etiquetas (y) ---\n",
    "\n",
    "LABEL_COLUMN_NAME = 'label' # (Tu 'value_counts' confirm√≥ que esto es correcto)\n",
    "\n",
    "# a. Guardar las etiquetas (y)\n",
    "# (Usamos .copy() para evitar advertencias de Pandas)\n",
    "y_data = df_processed[LABEL_COLUMN_NAME].copy()\n",
    "\n",
    "# b. Guardar las caracter√≠sticas (X)\n",
    "# (Todo lo que NO es la etiqueta)\n",
    "X_data = df_processed.drop(columns=[LABEL_COLUMN_NAME])\n",
    "\n",
    "print(f\"\\n--- Caracter√≠sticas (X) y Etiquetas (y) separadas ---\")\n",
    "print(f\"Forma de X_data: {X_data.shape}\")\n",
    "print(f\"Forma de y_data: {y_data.shape}\")\n",
    "\n",
    "\n",
    "# --- 5. ü§ñ Codificaci√≥n de Etiquetas (y) ---\n",
    "# Convertir 'BenignTraffic', 'DDoS-ICMP_Flood', etc., a n√∫meros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "y_final = le.fit_transform(y_data)\n",
    "\n",
    "# Imprimir las clases y sus n√∫meros (¬°muy √∫til!)\n",
    "print(\"\\n--- Mapeo de Clases (LabelEncoder) ---\")\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(class_mapping)\n",
    "\n",
    "\n",
    "# --- 6. ü§ñ Normalizaci√≥n (Scaling) de Caracter√≠sticas (X) ---\n",
    "# Las redes neuronales (TCN, LSTM) funcionan mejor con datos entre 0 y 1.\n",
    "# Como TUS datos ya son todos num√©ricos (float64),\n",
    "# ¬°podemos escalar todas las columnas de X_data!\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 'fit_transform' devuelve un array de NumPy, no un DataFrame\n",
    "X_scaled_array = scaler.fit_transform(X_data)\n",
    "\n",
    "# Lo convertimos de nuevo a DataFrame para verlo (mantenemos los nombres)\n",
    "X_final = pd.DataFrame(X_scaled_array, columns=X_data.columns)\n",
    "\n",
    "print(\"\\n--- Normalizaci√≥n (MinMaxScaler) completada ---\")\n",
    "print(X_final.head())\n",
    "\n",
    "\n",
    "# --- 7. ‚úÖ ¬°Listos para la Etapa 3! ---\n",
    "print(\"\\n--- ¬°Preprocesamiento de la muestra completado! ---\")\n",
    "print(\"Tenemos:\")\n",
    "print(f\"1. X_final (features, normalizadas): {X_final.shape}\")\n",
    "print(f\"2. y_final (labels, codificadas): {y_final.shape}\")\n",
    "print(f\"3. 'le' (el LabelEncoder): para decodificar las predicciones.\")\n",
    "print(f\"4. 'scaler' (el MinMaxScaler): para aplicar a nuevos datos.\")"
   ],
   "id": "1e4c8602f5ddd538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 3: Poda y Muestreo Estratificado\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Importaciones Adicionales ---\n",
    "import os\n",
    "from tqdm import tqdm # Para la barra de progreso\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. Par√°metros de Muestreo ---\n",
    "\n",
    "#################################################################\n",
    "# ¬°IMPORTANTE! Define cu√°ntas muestras quieres POR CLASE.\n",
    "# Un valor m√°s bajo = un dataset final m√°s peque√±o.\n",
    "# 10,000 es un buen punto de partida.\n",
    "#################################################################\n",
    "SAMPLES_PER_CLASS_LIMIT = 10000\n",
    "\n",
    "# (Asumimos que 'le' y 'scaler' de la Etapa 2 existen)\n",
    "# (Asumimos que 'csv_directory' de la Etapa 1 existe)\n",
    "# (Asumimos que 'new_columns' de la Etapa 2 existe)\n",
    "# 'new_columns' es la lista de columnas limpias, incluyendo 'label'\n",
    "# Si no la tienes, es:\n",
    "# new_columns = ['flow_duration', 'header_length', 'protocol_type', ..., 'weight', 'label']\n",
    "\n",
    "try:\n",
    "    # Verificamos que las variables clave existan\n",
    "    print(f\"Directorio de CSVs: {csv_directory}\")\n",
    "    print(f\"Codificador de etiquetas: {le}\")\n",
    "    print(f\"Escalador: {scaler}\")\n",
    "    print(f\"L√≠mite de muestras por clase: {SAMPLES_PER_CLASS_LIMIT}\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Parece que 'csv_directory', 'le', o 'scaler' no existen.\")\n",
    "    print(\"Por favor, ejecuta las Etapas 1 y 2 de nuevo antes de esta.\")\n",
    "    # Detener la ejecuci√≥n si faltan variables clave\n",
    "    raise\n",
    "\n",
    "# --- 3. Bucle Principal de Procesamiento ---\n",
    "\n",
    "# Lista para guardar los \"mini-dataframes\" muestreados de cada archivo\n",
    "list_of_sampled_dfs = []\n",
    "\n",
    "print(f\"\\nIniciando procesamiento de {len(all_csv_files)} archivos CSV...\")\n",
    "\n",
    "# Usamos tqdm para tener una barra de progreso\n",
    "for csv_file in tqdm(all_csv_files, desc=\"Procesando archivos\"):\n",
    "    file_path = os.path.join(csv_directory, csv_file)\n",
    "\n",
    "    try:\n",
    "        # 1. Cargar el archivo\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Omitir si el archivo est√° vac√≠o\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # 2. Aplicar el MISMO pipeline de limpieza de la Etapa 2\n",
    "\n",
    "        # a. Limpiar nombres de columnas\n",
    "        df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "        # b. Limpiar Infinitos y NaNs\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # c. Separar X e y\n",
    "        # Asegurarnos de que el archivo tenga la columna 'label'\n",
    "        if 'label' not in df.columns:\n",
    "            print(f\"Advertencia: {csv_file} no tiene columna 'label', omitiendo.\")\n",
    "            continue\n",
    "\n",
    "        y_temp_labels = df['label'].copy()\n",
    "        X_temp_features = df.drop(columns=['label'])\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # ¬°TRUCO CLAVE!\n",
    "        # Aqu√≠ NO usamos .fit_transform()\n",
    "        # Usamos .transform() para aplicar la MISMA escala\n",
    "        # que aprendimos del primer archivo (df_sample).\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        # d. Escalar X (features)\n",
    "        # Asegurarnos que las columnas est√°n en el mismo orden que el scaler espera\n",
    "        # (usamos 'X_final.columns' de la Etapa 2)\n",
    "        X_temp_features = X_temp_features[X_final.columns]\n",
    "        X_scaled = scaler.transform(X_temp_features)\n",
    "\n",
    "        # e. Codificar y (labels)\n",
    "        y_encoded = le.transform(y_temp_labels)\n",
    "\n",
    "        # 3. Re-unir todo para el muestreo\n",
    "        # Convertimos X de nuevo a DataFrame y a√±adimos y\n",
    "        df_processed = pd.DataFrame(X_scaled, columns=X_final.columns)\n",
    "        df_processed['label_encoded'] = y_encoded\n",
    "\n",
    "        # 4. Muestreo Estratificado (El paso de \"Poda\")\n",
    "        # Agrupar por la etiqueta codificada y tomar 'n' muestras\n",
    "        # (o todas las que haya si hay menos de 'n')\n",
    "        df_sampled = df_processed.groupby('label_encoded').apply(\n",
    "            lambda x: x.sample(n=min(len(x), SAMPLES_PER_CLASS_LIMIT))\n",
    "        ).reset_index(drop=True) # Resetear el √≠ndice\n",
    "\n",
    "        # 5. Guardar este \"mini-dataset\" en nuestra lista\n",
    "        list_of_sampled_dfs.append(df_sampled)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR al procesar el archivo {csv_file}: {e}\")\n",
    "        # (Esto puede pasar si un CSV est√° corrupto o tiene columnas diferentes)\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Procesamiento de archivos completado ---\")\n",
    "\n",
    "# --- 4. Creaci√≥n del Dataset Final ---\n",
    "\n",
    "if not list_of_sampled_dfs:\n",
    "    print(\"ERROR: No se proces√≥ ning√∫n archivo. La lista de DataFrames est√° vac√≠a.\")\n",
    "else:\n",
    "    print(\"Combinando todos los DataFrames muestreados...\")\n",
    "    # ¬°Combinamos los 169 peque√±os DataFrames en uno solo!\n",
    "    df_final_sampled = pd.concat(list_of_sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # ¬°Paso Cr√≠tico de Des-duplicaci√≥n!\n",
    "    # Es posible que la misma muestra (ej. un ataque raro)\n",
    "    # est√© en m√∫ltiples archivos.\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    df_final_sampled = df_final_sampled.drop_duplicates()\n",
    "\n",
    "    print(\"\\n--- ¬°Dataset Final Creado! ---\")\n",
    "    print(f\"Forma del dataset final: {df_final_sampled.shape}\")\n",
    "\n",
    "    # Veamos la nueva distribuci√≥n de clases\n",
    "    print(\"\\nDistribuci√≥n de clases en el dataset final:\")\n",
    "    # Usamos el 'le' para traducir los n√∫meros (0, 1, 2...)\n",
    "    # de vuelta a texto ('Backdoor', 'BenignTraffic'...)\n",
    "\n",
    "    # Invertir el mapeo para leer los nombres\n",
    "    # (El 'le.classes_[idx]' hace esto)\n",
    "    final_counts = df_final_sampled['label_encoded'].value_counts()\n",
    "    final_counts.index = final_counts.index.map(lambda idx: le.classes_[idx])\n",
    "    print(final_counts)\n",
    "\n",
    "    # --- 5. Guardar el resultado ---\n",
    "    # ¬°No queremos volver a hacer esto!\n",
    "    # Guardar en un formato r√°pido como Parquet o Feather.\n",
    "    # (Feather es a veces m√°s r√°pido y simple)\n",
    "\n",
    "    # Instalaci√≥n: pip install pyarrow\n",
    "    try:\n",
    "        df_final_sampled.to_feather('dataset_final_procesado.feather')\n",
    "        print(\"\\nDataset final guardado como 'dataset_final_procesado.feather'\")\n",
    "        print(\"En el futuro, solo necesitar√°s cargar este archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar: {e}. ¬øInstalaste 'pyarrow'?\")\n",
    "        # (Si falla, guardamos como CSV)\n",
    "        df_final_sampled.to_csv('dataset_final_procesado.csv', index=False)\n",
    "        print(\"\\nDataset final guardado como 'dataset_final_procesado.csv'\")"
   ],
   "id": "8c94eee685170c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 4: Carga y Muestreo Final (Train/Test Split)\n",
    "# ----------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow.feather as feather # Espec√≠fico para leer .feather\n",
    "\n",
    "# --- 1. Cargar el Dataset Procesado ---\n",
    "print(\"Cargando 'dataset_final_procesado.feather'...\")\n",
    "# (Esto puede tardar un poco y usar bastante RAM)\n",
    "try:\n",
    "    df_final = feather.read_feather('dataset_final_procesado.feather')\n",
    "    print(\"Dataset cargado exitosamente.\")\n",
    "    print(f\"Forma original: {df_final.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}. ¬øEst√°s seguro de que se guard√≥ como .feather?\")\n",
    "    # (Si guardaste como CSV, usa pd.read_csv)\n",
    "    # df_final = pd.read_csv('dataset_final_procesado.csv')\n",
    "\n",
    "\n",
    "# --- 2. Separar X (features) e y (labels) ---\n",
    "# La columna 'label_encoded' es nuestra 'y'\n",
    "y = df_final['label_encoded']\n",
    "\n",
    "# Todas las dem√°s columnas son nuestras 'X'\n",
    "X = df_final.drop(columns=['label_encoded'])\n",
    "\n",
    "print(\"Caracter√≠sticas (X) y Etiquetas (y) separadas.\")\n",
    "\n",
    "\n",
    "# --- 3. Muestreo Estratificado (El paso clave) ---\n",
    "\n",
    "# No podemos usar los 22.6M de filas.\n",
    "# Vamos a crear un set de entrenamiento y prueba mucho m√°s peque√±o.\n",
    "# Por ejemplo, usemos un 5% del total (aprox 1.1M de filas)\n",
    "# y mantengamos el 95% restante sin usar por ahora.\n",
    "\n",
    "# train_size=0.05 significa que nos quedamos con el 5%\n",
    "# test_size=0.01 significa que el 1% ser√° para validaci√≥n\n",
    "# (Esto divide el 5% en 4% train y 1% test, aprox)\n",
    "\n",
    "# 'stratify=y' es ESENCIAL.\n",
    "# Asegura que si 'Uploading_Attack' es el 0.001% de los datos,\n",
    "# tambi√©n ser√° el 0.001% de nuestro set de entrenamiento.\n",
    "\n",
    "print(\"Iniciando muestreo estratificado (train_test_split)...\")\n",
    "\n",
    "# Dividimos una primera vez para reducir el tama√±o\n",
    "X_muestra, _, y_muestra, _ = train_test_split(\n",
    "    X, y,\n",
    "    train_size=0.05,  # ¬°Tomar solo el 5% del total!\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Tama√±o de la muestra (5%): {X_muestra.shape}\")\n",
    "\n",
    "# Ahora dividimos esa muestra en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_muestra, y_muestra,\n",
    "    test_size=0.2, # 20% de la muestra para test (o sea, 1% del total)\n",
    "    shuffle=True,\n",
    "    stratify=y_muestra,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n--- ¬°Sets de Entrenamiento y Prueba Creados! ---\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Forma de X_test:  {X_test.shape}\")\n",
    "print(f\"Forma de y_test:  {y_test.shape}\")\n",
    "\n",
    "# (Opcional: liberar memoria)\n",
    "# del df_final, X, y\n",
    "# print(\"Memoria del DataFrame original liberada.\")"
   ],
   "id": "8bdffa026291853e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 5: Modelo Base (Random Forest) y M√©tricas\n",
    "# ----------------------------------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Asumimos que 'le' (LabelEncoder) de la Etapa 2 todav√≠a existe)\n",
    "# 'le.classes_' contiene los nombres (ej. 'BenignTraffic')\n",
    "\n",
    "# --- 1. Definir y Entrenar el Modelo ---\n",
    "print(\"\\n--- Iniciando Entrenamiento del Modelo Base (Random Forest) ---\")\n",
    "\n",
    "# n_jobs=-1 usa todos los cores de tu CPU para ir m√°s r√°pido\n",
    "# 'random_state=42' es para que el resultado sea reproducible\n",
    "rf_model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# ¬°El entrenamiento!\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"¬°Entrenamiento completado!\")\n",
    "\n",
    "\n",
    "# --- 2. Hacer Predicciones ---\n",
    "print(\"Realizando predicciones en el set de prueba (X_test)...\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# --- 3. Evaluar el Modelo ---\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificaci√≥n ---\")\n",
    "# 'target_names' usa nuestro 'le' para poner los nombres\n",
    "# de los ataques en el reporte.\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# --- 4. Visualizar la Matriz de Confusi√≥n ---\n",
    "print(\"Generando Matriz de Confusi√≥n...\")\n",
    "# (Esto puede ser un gr√°fico muy grande si hay muchas clases)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20)) # Ajustar tama√±o\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    ax=ax,\n",
    "    xticks_rotation='vertical',\n",
    "    normalize='true', # Muestra porcentajes\n",
    "    labels=le.transform(le.classes_), # Asegura el orden\n",
    "    display_labels=le.classes_\n",
    ")\n",
    "plt.title(\"Matriz de Confusi√≥n (Normalizada)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5ac539b23960e8da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 6: Remuestreo con SMOTE y Re-evaluaci√≥n (Corregido)\n",
    "# ----------------------------------------------\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# (Asumimos que 'X_train', 'y_train', 'X_test', 'y_test'\n",
    "#  y 'le' (LabelEncoder) de las etapas 4 y 5 todav√≠a existen)\n",
    "\n",
    "print(f\"Forma original de X_train: {X_train.shape}\")\n",
    "print(f\"Forma original de y_train: {y_train.shape}\")\n",
    "\n",
    "# --- 1. Configurar y Aplicar SMOTE ---\n",
    "# 'auto' sobremuestrea todas las clases minoritarias\n",
    "# para igualar a la clase mayoritaria.\n",
    "\n",
    "print(\"\\nIniciando SMOTE... (Esto puede tardar varios minutos)\")\n",
    "\n",
    "#################################################################\n",
    "# --- CORRECCI√ìN ---\n",
    "# Se elimin√≥ el argumento 'n_jobs=-1', que no es v√°lido para SMOTE.\n",
    "#################################################################\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "#################################################################\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"SMOTE completado en {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"\\nForma NUEVA de X_train (SMOTE): {X_train_smote.shape}\")\n",
    "print(f\"Forma NUEVA de y_train (SMOTE): {y_train_smote.shape}\")\n",
    "\n",
    "# --- 2. Entrenar el RandomForest en los datos CON SMOTE ---\n",
    "print(\"\\n--- Iniciando Entrenamiento del RandomForest (con SMOTE) ---\")\n",
    "# Aqu√≠ S√ç usamos n_jobs=-1\n",
    "rf_model_smote = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_model_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"¬°Entrenamiento (SMOTE) completado!\")\n",
    "\n",
    "\n",
    "# --- 3. Evaluar el Modelo (SMOTE) ---\n",
    "print(\"\\nRealizando predicciones en el set de prueba original (X_test)...\")\n",
    "y_pred_smote = rf_model_smote.predict(X_test)\n",
    "\n",
    "\n",
    "# --- 4. Reporte de Clasificaci√≥n (SMOTE) ---\n",
    "print(\"\\n--- Reporte de Clasificaci√≥n (Modelo con SMOTE) ---\")\n",
    "report_smote = classification_report(\n",
    "    y_test,\n",
    "    y_pred_smote,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report_smote)\n",
    "\n",
    "\n",
    "# --- 5. Matriz de Confusi√≥n (SMOTE) ---\n",
    "print(\"Generando Matriz de Confusi√≥n (Modelo con SMOTE)...\")\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_smote,\n",
    "    ax=ax,\n",
    "    xticks_rotation='vertical',\n",
    "    normalize='true',\n",
    "    labels=le.transform(le.classes_),\n",
    "    display_labels=le.classes_\n",
    ")\n",
    "plt.title(\"Matriz de Confusi√≥n (SMOTE - Normalizada)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "dbd6c5d44e03abff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# Celda de Verificaci√≥n de GPU (Antes de la Etapa 7)\n",
    "# ----------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Versi√≥n de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "# Esta es la funci√≥n clave\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if not gpu_devices:\n",
    "    print(\"\\n--- ¬°ADVERTENCIA! ---\")\n",
    "    print(\"TensorFlow NO PUEDE encontrar tu GPU (GTX 1650).\")\n",
    "    print(\"El entrenamiento de la Etapa 7 se ejecutar√° en el CPU (ser√° MUY lento).\")\n",
    "    print(\"Posible soluci√≥n: Debes instalar NVIDIA CUDA Toolkit y cuDNN.\")\n",
    "else:\n",
    "    print(\"\\n--- ¬°√âXITO! ---\")\n",
    "    print(f\"TensorFlow S√ç detect√≥ tu GPU:\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"  - {device.name}\")\n",
    "    print(\"\\nEl entrenamiento de la Etanpa 7 usar√° la GPU y ser√° mucho m√°s r√°pido.\")"
   ],
   "id": "8605f77ad7b5df61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 7: Modelo de IA (Temporal Convolutional Network - TCN)\n",
    "# ----------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tcn import TCN # Importar TCN\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "# (Asumimos que 'X_train_smote', 'y_train_smote', 'X_test', 'y_test'\n",
    "#  y 'le' (LabelEncoder) de las etapas anteriores todav√≠a existen)\n",
    "\n",
    "# --- 1. Preparar Datos para TCN (Reshape 2D -> 3D) ---\n",
    "print(\"--- Preparando datos para TCN (3D) ---\")\n",
    "\n",
    "# a. Obtener los valores de X (como arrays de NumPy)\n",
    "# Usamos .values para obtener los datos sin el √≠ndice de Pandas\n",
    "X_train_vals = X_train_smote.values\n",
    "X_test_vals = X_test.values\n",
    "\n",
    "# b. Obtener el n√∫mero de features y clases\n",
    "N_FEATURES = X_train_vals.shape[1] # Deber√≠a ser 46\n",
    "N_CLASSES = len(le.classes_)      # Deber√≠a ser 34\n",
    "TIMESTEPS = 1                     # Nuestro \"truco\"\n",
    "\n",
    "# c. Reformatear X a 3D: (muestras, timesteps, features)\n",
    "X_train_3D = X_train_vals.reshape((X_train_vals.shape[0], TIMESTEPS, N_FEATURES))\n",
    "X_test_3D = X_test_vals.reshape((X_test_vals.shape[0], TIMESTEPS, N_FEATURES))\n",
    "\n",
    "print(f\"Forma de X_train (original 2D): {X_train_vals.shape}\")\n",
    "print(f\"Forma de X_train (nueva 3D):  {X_train_3D.shape}\")\n",
    "print(f\"Forma de X_test (original 2D):  {X_test_vals.shape}\")\n",
    "print(f\"Forma de X_test (nueva 3D):   {X_test_3D.shape}\")\n",
    "\n",
    "# d. Codificar 'y' (One-Hot Encoding)\n",
    "# Keras prefiere que las clases (0, 1, 2... 33)\n",
    "# est√©n en formato \"categ√≥rico\" (One-Hot)\n",
    "y_train_cat = to_categorical(y_train_smote, num_classes=N_CLASSES)\n",
    "y_test_cat = to_categorical(y_test, num_classes=N_CLASSES)\n",
    "\n",
    "print(f\"\\nForma de y_train (categ√≥rica): {y_train_cat.shape}\")\n",
    "print(f\"Forma de y_test (categ√≥rica):  {y_test_cat.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Construir el Modelo TCN ---\n",
    "print(\"\\n--- Construyendo el modelo TCN ---\")\n",
    "\n",
    "# Aqu√≠ puedes \"jugar con los par√°metros\" que mencionaste\n",
    "model_tcn = Sequential([\n",
    "    TCN(\n",
    "        input_shape=(TIMESTEPS, N_FEATURES), # (1, 46)\n",
    "        nb_filters=64,       # N√∫mero de filtros (neuronas) en cada capa\n",
    "        kernel_size=2,       # Tama√±o del kernel de convoluci√≥n\n",
    "        dilations=[1, 2, 4], # Lista de dilataciones\n",
    "        nb_stacks=1,         # N√∫mero de \"bloques\" TCN\n",
    "        padding='causal',\n",
    "        use_skip_connections=True,\n",
    "        dropout_rate=0.1,    # Dropout para regularizaci√≥n\n",
    "        return_sequences=False # False porque queremos UNA salida (no una secuencia)\n",
    "    ),\n",
    "    Dropout(0.2), # Dropout adicional antes de la capa final\n",
    "    Dense(N_CLASSES, activation='softmax') # Capa final de 34 neuronas\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_tcn.compile(\n",
    "    optimizer='adam',                   # Optimizador popular\n",
    "    loss='categorical_crossentropy',    # P√©rdida para multi-clase\n",
    "    metrics=['accuracy']                # M√©trica a monitorear\n",
    ")\n",
    "\n",
    "model_tcn.summary()\n",
    "\n",
    "\n",
    "# --- 3. Entrenar el Modelo TCN ---\n",
    "print(\"\\n--- Iniciando Entrenamiento de la TCN ---\")\n",
    "print(\"Esto puede tardar bastante...\")\n",
    "\n",
    "# (Si tienes una GPU, TensorFlow la usar√° autom√°ticamente)\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenamos el modelo.\n",
    "# batch_size=256 o 512 es bueno para datasets grandes\n",
    "history = model_tcn.fit(\n",
    "    X_train_3D,\n",
    "    y_train_cat,\n",
    "    epochs=10, # 10 \"pasadas\" por los datos. Puedes subirlo si tienes tiempo.\n",
    "    batch_size=512,\n",
    "    validation_data=(X_test_3D, y_test_cat) # Validamos con los datos de test\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Entrenamiento de TCN completado en {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "\n",
    "# --- 4. Evaluar el Modelo TCN ---\n",
    "print(\"\\n--- Reporte de Clasificaci√≥n (Modelo TCN) ---\")\n",
    "\n",
    "# Predecir sobre X_test_3D\n",
    "y_pred_probs = model_tcn.predict(X_test_3D)\n",
    "\n",
    "# Las predicciones 'y_pred_probs' son probabilidades.\n",
    "# Necesitamos la clase con la probabilidad m√°s alta (argmax)\n",
    "y_pred_tcn = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Generar el reporte\n",
    "# Comparamos 'y_test' (los n√∫meros 0-33) con 'y_pred_tcn'\n",
    "report_tcn = classification_report(\n",
    "    y_test,\n",
    "    y_pred_tcn,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report_tcn)"
   ],
   "id": "bfd63b82333881e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
