{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 1 (Corregida): Carga, Descarga y AnÃ¡lisis (EDA)\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. ImportaciÃ³n de LibrerÃ­as ---\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"LibrerÃ­as importadas correctamente.\")\n",
    "\n",
    "# ConfiguraciÃ³n de Pandas y Matplotlib\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# --- 2. Descarga del Dataset (Usando CachÃ©) ---\n",
    "print(\"\\nLocalizando el dataset (usarÃ¡ la cachÃ© si ya existe)...\")\n",
    "download_path = \"\"\n",
    "try:\n",
    "    # -----------------------------------------------------------------\n",
    "    # NOTA: Esta funciÃ³n es 'cache-aware'.\n",
    "    # Si ya descargaste el dataset, NO lo volverÃ¡ a bajar.\n",
    "    # Simplemente te darÃ¡ la ruta a los archivos existentes en cachÃ©.\n",
    "    # -----------------------------------------------------------------\n",
    "    download_path = kagglehub.dataset_download(\"madhavmalhotra/unb-cic-iot-dataset\")\n",
    "    print(f\"Dataset (desde cachÃ© o descarga) localizado en: {download_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR al descargar: {e}\")\n",
    "    print(\"Por favor, verifica tu conexiÃ³n y que 'kaggle.json' estÃ© en el lugar correcto.\")\n",
    "\n",
    "\n",
    "# --- 3. BÃºsqueda y Carga del CSV ---\n",
    "# Esta es la ruta base que devuelve kagglehub\n",
    "base_download_path = download_path\n",
    "\n",
    "# Construimos la ruta correcta que descubrimos\n",
    "csv_directory = os.path.join(base_download_path, 'wataiData', 'csv', 'CICIoT2023')\n",
    "print(f\"\\nAccediendo al directorio de CSVs: {csv_directory}\")\n",
    "\n",
    "csv_file_path = \"\"\n",
    "df_sample = None\n",
    "\n",
    "try:\n",
    "    # Listar todos los archivos en ese directorio\n",
    "    all_csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "    if not all_csv_files:\n",
    "        print(\"ERROR: No se encontraron archivos .csv en el directorio especificado.\")\n",
    "    else:\n",
    "        print(f\"Â¡Ã‰xito! Se encontraron {len(all_csv_files)} archivos CSV.\")\n",
    "\n",
    "        # --- 4. Carga de MUESTRA (Sampling) ---\n",
    "        # No cargaremos los 169 archivos.\n",
    "        # Para el anÃ¡lisis, cargaremos solo el PRIMER archivo de la lista.\n",
    "\n",
    "        first_csv_file = all_csv_files[0]\n",
    "        csv_file_path = os.path.join(csv_directory, first_csv_file)\n",
    "\n",
    "        print(f\"\\nCargando el primer archivo como muestra: {first_csv_file}...\")\n",
    "\n",
    "        df_sample = pd.read_csv(csv_file_path)\n",
    "\n",
    "        print(\"Muestra cargada correctamente.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: El directorio {csv_directory} no existe. Verifica la ruta.\")\n",
    "except Exception as e:\n",
    "    print(f\"OcurriÃ³ un error al listar o cargar archivos: {e}\")\n",
    "\n",
    "\n",
    "# --- 5. AnÃ¡lisis de Clases (Si la carga fue exitosa) ---\n",
    "if df_sample is not None:\n",
    "    try:\n",
    "        print(\"\\n--- InformaciÃ³n General del Dataset (Muestra del primer CSV) ---\")\n",
    "        df_sample.info()\n",
    "\n",
    "        print(\"\\n--- Primeras 5 filas (para ver columnas) ---\")\n",
    "        print(df_sample.head())\n",
    "\n",
    "        #################################################################\n",
    "        # Â¡IMPORTANTE!\n",
    "        # Revisa la salida de 'df_sample.head()' y confirma\n",
    "        # el nombre de la columna de etiquetas.\n",
    "        # CÃ¡mbialo aquÃ­ si es necesario.\n",
    "        LABEL_COLUMN_NAME = 'label'\n",
    "        #################################################################\n",
    "\n",
    "        if LABEL_COLUMN_NAME in df_sample.columns:\n",
    "            print(f\"\\n--- DistribuciÃ³n de Clases (Columna: '{LABEL_COLUMN_NAME}') ---\")\n",
    "            class_distribution = df_sample[LABEL_COLUMN_NAME].value_counts()\n",
    "            print(class_distribution)\n",
    "\n",
    "            # --- 6. VisualizaciÃ³n de Clases ---\n",
    "            print(\"\\nGenerando grÃ¡fico de distribuciÃ³n de clases...\")\n",
    "            plt.figure(figsize=(12, 10))\n",
    "\n",
    "            sns.countplot(y=df_sample[LABEL_COLUMN_NAME],\n",
    "                          order=class_distribution.index)\n",
    "\n",
    "            plt.title(f'DistribuciÃ³n de Clases (Archivo: {first_csv_file})')\n",
    "            plt.xlabel('Cantidad de Muestras (Escala LogarÃ­tmica)')\n",
    "            plt.ylabel('Tipo de TrÃ¡fico (Ataque / Benigno)')\n",
    "            plt.xscale('log')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n--- ERROR DE ANÃLISIS ---\")\n",
    "            print(f\"No se encontrÃ³ la columna '{LABEL_COLUMN_NAME}'.\")\n",
    "            print(f\"Las columnas disponibles son: {df_sample.columns.tolist()}\")\n",
    "            print(\"Por favor, actualiza la variable 'LABEL_COLUMN_NAME' en el script.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OcurriÃ³ un error durante el anÃ¡lisis: {e}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 2 Preprocesamiento y Limpieza\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Importaciones Adicionales ---\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# (Asumimos que 'df_sample' todavÃ­a existe en memoria de la Etapa 1)\n",
    "# Si no, vuelve a cargar el primer CSV:\n",
    "# csv_file_path = ... (ruta al primer csv)\n",
    "# df_sample = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(f\"--- DataFrame Original (Muestra) ---\")\n",
    "print(f\"Forma original: {df_sample.shape}\")\n",
    "\n",
    "# --- 2. ðŸ§¹ Limpieza de Nombres de Columnas ---\n",
    "# Â¡IMPORTANTE! Arreglamos los nombres con espacios (ej. 'Protocol Type')\n",
    "# Reemplazamos ' ' por '_' y convertimos a minÃºsculas\n",
    "original_columns = df_sample.columns.tolist()\n",
    "df_sample.columns = [col.strip().replace(' ', '_').lower() for col in df_sample.columns]\n",
    "new_columns = df_sample.columns.tolist()\n",
    "\n",
    "print(\"\\n--- Nombres de Columnas Corregidos ---\")\n",
    "print(\"Originales:\", original_columns)\n",
    "print(\"Nuevos:\", new_columns)\n",
    "\n",
    "\n",
    "# --- 3. ðŸ§¹ Limpieza de Datos (NaN e Infinitos) ---\n",
    "# Aunque .info() no mostrÃ³ NaNs, los valores Infinitos (Inf)\n",
    "# pueden existir y no se reportan como NaN.\n",
    "# Reemplazamos Inf y -Inf con NaN\n",
    "df_processed = df_sample.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Ahora, si ese reemplazo creÃ³ NaNs, los llenamos con 0.\n",
    "# (Llenar con 0 es seguro en datos de red)\n",
    "df_processed = df_processed.fillna(0)\n",
    "print(\"\\n--- Limpieza de Infinitos y NaNs completada ---\")\n",
    "\n",
    "\n",
    "# --- 4. ðŸ“Š SeparaciÃ³n de CaracterÃ­sticas (X) y Etiquetas (y) ---\n",
    "\n",
    "LABEL_COLUMN_NAME = 'label' # (Tu 'value_counts' confirmÃ³ que esto es correcto)\n",
    "\n",
    "# a. Guardar las etiquetas (y)\n",
    "# (Usamos .copy() para evitar advertencias de Pandas)\n",
    "y_data = df_processed[LABEL_COLUMN_NAME].copy()\n",
    "\n",
    "# b. Guardar las caracterÃ­sticas (X)\n",
    "# (lo que NO es la etiqueta)\n",
    "X_data = df_processed.drop(columns=[LABEL_COLUMN_NAME])\n",
    "\n",
    "print(f\"\\n--- CaracterÃ­sticas (X) y Etiquetas (y) separadas ---\")\n",
    "print(f\"Forma de X_data: {X_data.shape}\")\n",
    "print(f\"Forma de y_data: {y_data.shape}\")\n",
    "\n",
    "\n",
    "# --- 5. ðŸ¤– CodificaciÃ³n de Etiquetas (y) ---\n",
    "# Convertir 'BenignTraffic', 'DDoS-ICMP_Flood', etc., a nÃºmeros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "y_final = le.fit_transform(y_data)\n",
    "\n",
    "# Imprimir las clases y sus nÃºmeros (Â¡muy Ãºtil!)\n",
    "print(\"\\n--- Mapeo de Clases (LabelEncoder) ---\")\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(class_mapping)\n",
    "\n",
    "\n",
    "# --- 6. ðŸ¤– NormalizaciÃ³n (Scaling) de CaracterÃ­sticas (X) ---\n",
    "# Las redes neuronales (TCN, LSTM) funcionan mejor con datos entre 0 y 1.\n",
    "# Como TUS datos ya son todos numÃ©ricos (float64),\n",
    "# Â¡podemos escalar todas las columnas de X_data!\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 'fit_transform' devuelve un array de NumPy, no un DataFrame\n",
    "X_scaled_array = scaler.fit_transform(X_data)\n",
    "\n",
    "# Lo convertimos de nuevo a DataFrame para verlo (mantenemos los nombres)\n",
    "X_final = pd.DataFrame(X_scaled_array, columns=X_data.columns)\n",
    "\n",
    "print(\"\\n--- NormalizaciÃ³n (MinMaxScaler) completada ---\")\n",
    "print(X_final.head())\n",
    "\n",
    "\n",
    "# --- 7. âœ… Â¡Listos para la Etapa 3! ---\n",
    "print(\"\\n--- Â¡Preprocesamiento de la muestra completado! ---\")\n",
    "print(\"Tenemos:\")\n",
    "print(f\"1. X_final (features, normalizadas): {X_final.shape}\")\n",
    "print(f\"2. y_final (labels, codificadas): {y_final.shape}\")\n",
    "print(f\"3. 'le' (el LabelEncoder): para decodificar las predicciones.\")\n",
    "print(f\"4. 'scaler' (el MinMaxScaler): para aplicar a nuevos datos.\")"
   ],
   "id": "1e4c8602f5ddd538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 3: Poda y Muestreo Estratificado\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- 1. Importaciones Adicionales ---\n",
    "import os\n",
    "from tqdm import tqdm # Para la barra de progreso\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. ParÃ¡metros de Muestreo ---\n",
    "\n",
    "#################################################################\n",
    "# Â¡IMPORTANTE! Define cuÃ¡ntas muestras quieres POR CLASE.\n",
    "# Un valor mÃ¡s bajo = un dataset final mÃ¡s pequeÃ±o.\n",
    "# 10,000 es un buen punto de partida.\n",
    "#################################################################\n",
    "SAMPLES_PER_CLASS_LIMIT = 10000\n",
    "\n",
    "# (Asumimos que 'le' y 'scaler' de la Etapa 2 existen)\n",
    "# (Asumimos que 'csv_directory' de la Etapa 1 existe)\n",
    "# (Asumimos que 'new_columns' de la Etapa 2 existe)\n",
    "# 'new_columns' es la lista de columnas limpias, incluyendo 'label'\n",
    "# Si no la tienes, es:\n",
    "# new_columns = ['flow_duration', 'header_length', 'protocol_type', ..., 'weight', 'label']\n",
    "\n",
    "try:\n",
    "    # Verificamos que las variables clave existan\n",
    "    print(f\"Directorio de CSVs: {csv_directory}\")\n",
    "    print(f\"Codificador de etiquetas: {le}\")\n",
    "    print(f\"Escalador: {scaler}\")\n",
    "    print(f\"LÃ­mite de muestras por clase: {SAMPLES_PER_CLASS_LIMIT}\")\n",
    "except NameError:\n",
    "    print(\"ERROR: Parece que 'csv_directory', 'le', o 'scaler' no existen.\")\n",
    "    print(\"Por favor, ejecuta las Etapas 1 y 2 de nuevo antes de esta.\")\n",
    "    # Detener la ejecuciÃ³n si faltan variables clave\n",
    "    raise\n",
    "\n",
    "# --- 3. Bucle Principal de Procesamiento ---\n",
    "\n",
    "# Lista para guardar los \"mini-dataframes\" muestreados de cada archivo\n",
    "list_of_sampled_dfs = []\n",
    "\n",
    "print(f\"\\nIniciando procesamiento de {len(all_csv_files)} archivos CSV...\")\n",
    "\n",
    "# Usamos tqdm para tener una barra de progreso\n",
    "for csv_file in tqdm(all_csv_files, desc=\"Procesando archivos\"):\n",
    "    file_path = os.path.join(csv_directory, csv_file)\n",
    "\n",
    "    try:\n",
    "        # 1. Cargar el archivo\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Omitir si el archivo estÃ¡ vacÃ­o\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # 2. Aplicar el MISMO pipeline de limpieza de la Etapa 2\n",
    "\n",
    "        # a. Limpiar nombres de columnas\n",
    "        df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]\n",
    "\n",
    "        # b. Limpiar Infinitos y NaNs\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # c. Separar X e y\n",
    "        # Asegurarnos de que el archivo tenga la columna 'label'\n",
    "        if 'label' not in df.columns:\n",
    "            print(f\"Advertencia: {csv_file} no tiene columna 'label', omitiendo.\")\n",
    "            continue\n",
    "\n",
    "        y_temp_labels = df['label'].copy()\n",
    "        X_temp_features = df.drop(columns=['label'])\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Â¡TRUCO CLAVE!\n",
    "        # AquÃ­ NO usamos .fit_transform()\n",
    "        # Usamos .transform() para aplicar la MISMA escala\n",
    "        # que aprendimos del primer archivo (df_sample).\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        # d. Escalar X (features)\n",
    "        # Asegurarnos que las columnas estÃ¡n en el mismo orden que el scaler espera\n",
    "        # (usamos 'X_final.columns' de la Etapa 2)\n",
    "        X_temp_features = X_temp_features[X_final.columns]\n",
    "        X_scaled = scaler.transform(X_temp_features)\n",
    "\n",
    "        # e. Codificar y (labels)\n",
    "        y_encoded = le.transform(y_temp_labels)\n",
    "\n",
    "        # 3. Re-unir  para el muestreo\n",
    "        # Convertimos X de nuevo a DataFrame y aÃ±adimos y\n",
    "        df_processed = pd.DataFrame(X_scaled, columns=X_final.columns)\n",
    "        df_processed['label_encoded'] = y_encoded\n",
    "\n",
    "        # 4. Muestreo Estratificado (El paso de \"Poda\")\n",
    "        # Agrupar por la etiqueta codificada y tomar 'n' muestras\n",
    "        # (o todas las que haya si hay menos de 'n')\n",
    "        df_sampled = df_processed.groupby('label_encoded').apply(\n",
    "            lambda x: x.sample(n=min(len(x), SAMPLES_PER_CLASS_LIMIT))\n",
    "        ).reset_index(drop=True) # Resetear el Ã­ndice\n",
    "\n",
    "        # 5. Guardar este \"mini-dataset\" en nuestra lista\n",
    "        list_of_sampled_dfs.append(df_sampled)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR al procesar el archivo {csv_file}: {e}\")\n",
    "        # (Esto puede pasar si un CSV estÃ¡ corrupto o tiene columnas diferentes)\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Procesamiento de archivos completado ---\")\n",
    "\n",
    "# --- 4. CreaciÃ³n del Dataset Final ---\n",
    "\n",
    "if not list_of_sampled_dfs:\n",
    "    print(\"ERROR: No se procesÃ³ ningÃºn archivo. La lista de DataFrames estÃ¡ vacÃ­a.\")\n",
    "else:\n",
    "    print(\"Combinando todos los DataFrames muestreados...\")\n",
    "    # Â¡Combinamos los 169 pequeÃ±os DataFrames en uno solo!\n",
    "    df_final_sampled = pd.concat(list_of_sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Â¡Paso CrÃ­tico de Des-duplicaciÃ³n!\n",
    "    # Es posible que la misma muestra (ej. un ataque raro)\n",
    "    # estÃ© en mÃºltiples archivos.\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    df_final_sampled = df_final_sampled.drop_duplicates()\n",
    "\n",
    "    print(\"\\n--- Â¡Dataset Final Creado! ---\")\n",
    "    print(f\"Forma del dataset final: {df_final_sampled.shape}\")\n",
    "\n",
    "    # Veamos la nueva distribuciÃ³n de clases\n",
    "    print(\"\\nDistribuciÃ³n de clases en el dataset final:\")\n",
    "    # Usamos el 'le' para traducir los nÃºmeros (0, 1, 2...)\n",
    "    # de vuelta a texto ('Backdoor', 'BenignTraffic'...)\n",
    "\n",
    "    # Invertir el mapeo para leer los nombres\n",
    "    # (El 'le.classes_[idx]' hace esto)\n",
    "    final_counts = df_final_sampled['label_encoded'].value_counts()\n",
    "    final_counts.index = final_counts.index.map(lambda idx: le.classes_[idx])\n",
    "    print(final_counts)\n",
    "\n",
    "    # --- 5. Guardar el resultado ---\n",
    "    # Â¡No queremos volver a hacer esto!\n",
    "    # Guardar en un formato rÃ¡pido como Parquet o Feather.\n",
    "    # (Feather es a veces mÃ¡s rÃ¡pido y simple)\n",
    "\n",
    "    # InstalaciÃ³n: pip install pyarrow\n",
    "    try:\n",
    "        df_final_sampled.to_feather('dataset_final_procesado.feather')\n",
    "        print(\"\\nDataset final guardado como 'dataset_final_procesado.feather'\")\n",
    "        print(\"En el futuro, solo necesitarÃ¡s cargar este archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar: {e}. Â¿Instalaste 'pyarrow'?\")\n",
    "        # (Si falla, guardamos como CSV)\n",
    "        df_final_sampled.to_csv('dataset_final_procesado.csv', index=False)\n",
    "        print(\"\\nDataset final guardado como 'dataset_final_procesado.csv'\")"
   ],
   "id": "8c94eee685170c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 4: Carga y Muestreo Final (Train/Test Split)\n",
    "# ----------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow.feather as feather # EspecÃ­fico para leer .feather\n",
    "\n",
    "# --- 1. Cargar el Dataset Procesado ---\n",
    "print(\"Cargando 'dataset_final_procesado.feather'...\")\n",
    "# (Esto puede tardar un poco y usar bastante RAM)\n",
    "try:\n",
    "    df_final = feather.read_feather('dataset_final_procesado.feather')\n",
    "    print(\"Dataset cargado exitosamente.\")\n",
    "    print(f\"Forma original: {df_final.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}. Â¿EstÃ¡s seguro de que se guardÃ³ como .feather?\")\n",
    "    # (Si guardaste como CSV, usa pd.read_csv)\n",
    "    # df_final = pd.read_csv('dataset_final_procesado.csv')\n",
    "\n",
    "\n",
    "# --- 2. Separar X (features) e y (labels) ---\n",
    "# La columna 'label_encoded' es nuestra 'y'\n",
    "y = df_final['label_encoded']\n",
    "\n",
    "# Todas las demÃ¡s columnas son nuestras 'X'\n",
    "X = df_final.drop(columns=['label_encoded'])\n",
    "\n",
    "print(\"CaracterÃ­sticas (X) y Etiquetas (y) separadas.\")\n",
    "\n",
    "\n",
    "# --- 3. Muestreo Estratificado (El paso clave) ---\n",
    "\n",
    "# No podemos usar los 22.6M de filas.\n",
    "# Vamos a crear un set de entrenamiento y prueba mucho mÃ¡s pequeÃ±o.\n",
    "# Por ejemplo, usemos un 5% del total (aprox 1.1M de filas)\n",
    "# y mantengamos el 95% restante sin usar por ahora.\n",
    "\n",
    "# train_size=0.05 significa que nos quedamos con el 5%\n",
    "# test_size=0.01 significa que el 1% serÃ¡ para validaciÃ³n\n",
    "# (Esto divide el 5% en 4% train y 1% test, aprox)\n",
    "\n",
    "# 'stratify=y' es ESENCIAL.\n",
    "# Asegura que si 'Uploading_Attack' es el 0.001% de los datos,\n",
    "# tambiÃ©n serÃ¡ el 0.001% de nuestro set de entrenamiento.\n",
    "\n",
    "print(\"Iniciando muestreo estratificado (train_test_split)...\")\n",
    "\n",
    "# Dividimos una primera vez para reducir el tamaÃ±o\n",
    "X_muestra, _, y_muestra, _ = train_test_split(\n",
    "    X, y,\n",
    "    train_size=0.05,  # Â¡Tomar solo el 5% del total!\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TamaÃ±o de la muestra (5%): {X_muestra.shape}\")\n",
    "\n",
    "# Ahora dividimos esa muestra en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_muestra, y_muestra,\n",
    "    test_size=0.2, # 20% de la muestra para test (o sea, 1% del total)\n",
    "    shuffle=True,\n",
    "    stratify=y_muestra,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n--- Â¡Sets de Entrenamiento y Prueba Creados! ---\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Forma de X_test:  {X_test.shape}\")\n",
    "print(f\"Forma de y_test:  {y_test.shape}\")\n",
    "\n",
    "# (Opcional: liberar memoria)\n",
    "# del df_final, X, y\n",
    "# print(\"Memoria del DataFrame original liberada.\")"
   ],
   "id": "8bdffa026291853e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 5: Modelo Base (Random Forest) y MÃ©tricas\n",
    "# ----------------------------------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Asumimos que 'le' (LabelEncoder) de la Etapa 2 todavÃ­a existe)\n",
    "# 'le.classes_' contiene los nombres (ej. 'BenignTraffic')\n",
    "\n",
    "# --- 1. Definir y Entrenar el Modelo ---\n",
    "print(\"\\n--- Iniciando Entrenamiento del Modelo Base (Random Forest) ---\")\n",
    "\n",
    "# n_jobs=-1 usa todos los cores de tu CPU para ir mÃ¡s rÃ¡pido\n",
    "# 'random_state=42' es para que el resultado sea reproducible\n",
    "rf_model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Â¡El entrenamiento!\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Â¡Entrenamiento completado!\")\n",
    "\n",
    "\n",
    "# --- 2. Hacer Predicciones ---\n",
    "print(\"Realizando predicciones en el set de prueba (X_test)...\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# --- 3. Evaluar el Modelo ---\n",
    "\n",
    "print(\"\\n--- Reporte de ClasificaciÃ³n ---\")\n",
    "# 'target_names' usa nuestro 'le' para poner los nombres\n",
    "# de los ataques en el reporte.\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# --- 4. Visualizar la Matriz de ConfusiÃ³n ---\n",
    "print(\"Generando Matriz de ConfusiÃ³n...\")\n",
    "# (Esto puede ser un grÃ¡fico muy grande si hay muchas clases)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20)) # Ajustar tamaÃ±o\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    ax=ax,\n",
    "    xticks_rotation='vertical',\n",
    "    normalize='true', # Muestra porcentajes\n",
    "    labels=le.transform(le.classes_), # Asegura el orden\n",
    "    display_labels=le.classes_\n",
    ")\n",
    "plt.title(\"Matriz de ConfusiÃ³n (Normalizada)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5ac539b23960e8da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 6: Remuestreo con SMOTE y Re-evaluaciÃ³n (Corregido)\n",
    "# ----------------------------------------------\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# (Asumimos que 'X_train', 'y_train', 'X_test', 'y_test'\n",
    "#  y 'le' (LabelEncoder) de las etapas 4 y 5 todavÃ­a existen)\n",
    "\n",
    "print(f\"Forma original de X_train: {X_train.shape}\")\n",
    "print(f\"Forma original de y_train: {y_train.shape}\")\n",
    "\n",
    "# --- 1. Configurar y Aplicar SMOTE ---\n",
    "# 'auto' sobremuestrea todas las clases minoritarias\n",
    "# para igualar a la clase mayoritaria.\n",
    "\n",
    "print(\"\\nIniciando SMOTE... (Esto puede tardar varios minutos)\")\n",
    "\n",
    "#################################################################\n",
    "# --- CORRECCIÃ“N ---\n",
    "# Se eliminÃ³ el argumento 'n_jobs=-1', que no es vÃ¡lido para SMOTE.\n",
    "#################################################################\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "#################################################################\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"SMOTE completado en {end_time - start_time:.2f} segundos.\")\n",
    "print(f\"\\nForma NUEVA de X_train (SMOTE): {X_train_smote.shape}\")\n",
    "print(f\"Forma NUEVA de y_train (SMOTE): {y_train_smote.shape}\")\n",
    "\n",
    "# --- 2. Entrenar el RandomForest en los datos CON SMOTE ---\n",
    "print(\"\\n--- Iniciando Entrenamiento del RandomForest (con SMOTE) ---\")\n",
    "# AquÃ­ SÃ usamos n_jobs=-1\n",
    "rf_model_smote = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_model_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"Â¡Entrenamiento (SMOTE) completado!\")\n",
    "\n",
    "\n",
    "# --- 3. Evaluar el Modelo (SMOTE) ---\n",
    "print(\"\\nRealizando predicciones en el set de prueba original (X_test)...\")\n",
    "y_pred_smote = rf_model_smote.predict(X_test)\n",
    "\n",
    "\n",
    "# --- 4. Reporte de ClasificaciÃ³n (SMOTE) ---\n",
    "print(\"\\n--- Reporte de ClasificaciÃ³n (Modelo con SMOTE) ---\")\n",
    "report_smote = classification_report(\n",
    "    y_test,\n",
    "    y_pred_smote,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report_smote)\n",
    "\n",
    "\n",
    "# --- 5. Matriz de ConfusiÃ³n (SMOTE) ---\n",
    "print(\"Generando Matriz de ConfusiÃ³n (Modelo con SMOTE)...\")\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_smote,\n",
    "    ax=ax,\n",
    "    xticks_rotation='vertical',\n",
    "    normalize='true',\n",
    "    labels=le.transform(le.classes_),\n",
    "    display_labels=le.classes_\n",
    ")\n",
    "plt.title(\"Matriz de ConfusiÃ³n (SMOTE - Normalizada)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "dbd6c5d44e03abff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# Celda de VerificaciÃ³n de GPU (Antes de la Etapa 7)\n",
    "# ----------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"VersiÃ³n de TensorFlow: {tf.__version__}\")\n",
    "\n",
    "# Esta es la funciÃ³n clave\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if not gpu_devices:\n",
    "    print(\"\\n--- Â¡ADVERTENCIA! ---\")\n",
    "    print(\"TensorFlow NO PUEDE encontrar tu GPU (GTX 1650).\")\n",
    "    print(\"El entrenamiento de la Etapa 7 se ejecutarÃ¡ en el CPU (serÃ¡ MUY lento).\")\n",
    "    print(\"Posible soluciÃ³n: Debes instalar NVIDIA CUDA Toolkit y cuDNN.\")\n",
    "else:\n",
    "    print(\"\\n--- Â¡Ã‰XITO! ---\")\n",
    "    print(f\"TensorFlow SÃ detectÃ³ tu GPU:\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"  - {device.name}\")\n",
    "    print(\"\\nEl entrenamiento de la Etanpa 7 usarÃ¡ la GPU y serÃ¡ mucho mÃ¡s rÃ¡pido.\")"
   ],
   "id": "8605f77ad7b5df61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 7: Modelo de IA (Temporal Convolutional Network - TCN)\n",
    "# ----------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tcn import TCN # Importar TCN\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "# (Asumimos que 'X_train_smote', 'y_train_smote', 'X_test', 'y_test'\n",
    "#  y 'le' (LabelEncoder) de las etapas anteriores todavÃ­a existen)\n",
    "\n",
    "# --- 1. Preparar Datos para TCN (Reshape 2D -> 3D) ---\n",
    "print(\"--- Preparando datos para TCN (3D) ---\")\n",
    "\n",
    "# a. Obtener los valores de X (como arrays de NumPy)\n",
    "# Usamos .values para obtener los datos sin el Ã­ndice de Pandas\n",
    "X_train_vals = X_train_smote.values\n",
    "X_test_vals = X_test.values\n",
    "\n",
    "# b. Obtener el nÃºmero de features y clases\n",
    "N_FEATURES = X_train_vals.shape[1] # DeberÃ­a ser 46\n",
    "N_CLASSES = len(le.classes_)      # DeberÃ­a ser 34\n",
    "TIMESTEPS = 1                     # Nuestro \"truco\"\n",
    "\n",
    "# c. Reformatear X a 3D: (muestras, timesteps, features)\n",
    "X_train_3D = X_train_vals.reshape((X_train_vals.shape[0], TIMESTEPS, N_FEATURES))\n",
    "X_test_3D = X_test_vals.reshape((X_test_vals.shape[0], TIMESTEPS, N_FEATURES))\n",
    "\n",
    "print(f\"Forma de X_train (original 2D): {X_train_vals.shape}\")\n",
    "print(f\"Forma de X_train (nueva 3D):  {X_train_3D.shape}\")\n",
    "print(f\"Forma de X_test (original 2D):  {X_test_vals.shape}\")\n",
    "print(f\"Forma de X_test (nueva 3D):   {X_test_3D.shape}\")\n",
    "\n",
    "# d. Codificar 'y' (One-Hot Encoding)\n",
    "# Keras prefiere que las clases (0, 1, 2... 33)\n",
    "# estÃ©n en formato \"categÃ³rico\" (One-Hot)\n",
    "y_train_cat = to_categorical(y_train_smote, num_classes=N_CLASSES)\n",
    "y_test_cat = to_categorical(y_test, num_classes=N_CLASSES)\n",
    "\n",
    "print(f\"\\nForma de y_train (categÃ³rica): {y_train_cat.shape}\")\n",
    "print(f\"Forma de y_test (categÃ³rica):  {y_test_cat.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Construir el Modelo TCN ---\n",
    "print(\"\\n--- Construyendo el modelo TCN ---\")\n",
    "\n",
    "# AquÃ­ puedes \"jugar con los parÃ¡metros\" que mencionaste\n",
    "model_tcn = Sequential([\n",
    "    TCN(\n",
    "        input_shape=(TIMESTEPS, N_FEATURES), # (1, 46)\n",
    "        nb_filters=64,       # NÃºmero de filtros (neuronas) en cada capa\n",
    "        kernel_size=2,       # TamaÃ±o del kernel de convoluciÃ³n\n",
    "        dilations=[1, 2, 4], # Lista de dilataciones\n",
    "        nb_stacks=1,         # NÃºmero de \"bloques\" TCN\n",
    "        padding='causal',\n",
    "        use_skip_connections=True,\n",
    "        dropout_rate=0.1,    # Dropout para regularizaciÃ³n\n",
    "        return_sequences=False # False porque queremos UNA salida (no una secuencia)\n",
    "    ),\n",
    "    Dropout(0.2), # Dropout adicional antes de la capa final\n",
    "    Dense(N_CLASSES, activation='softmax') # Capa final de 34 neuronas\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_tcn.compile(\n",
    "    optimizer='adam',                   # Optimizador popular\n",
    "    loss='categorical_crossentropy',    # PÃ©rdida para multi-clase\n",
    "    metrics=['accuracy']                # MÃ©trica a monitorear\n",
    ")\n",
    "\n",
    "model_tcn.summary()\n",
    "\n",
    "\n",
    "# --- 3. Entrenar el Modelo TCN ---\n",
    "print(\"\\n--- Iniciando Entrenamiento de la TCN ---\")\n",
    "print(\"Esto puede tardar bastante...\")\n",
    "\n",
    "# (Si tienes una GPU, TensorFlow la usarÃ¡ automÃ¡ticamente)\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenamos el modelo.\n",
    "# batch_size=256 o 512 es bueno para datasets grandes\n",
    "history = model_tcn.fit(\n",
    "    X_train_3D,\n",
    "    y_train_cat,\n",
    "    epochs=10, # 10 \"pasadas\" por los datos. Puedes subirlo si tienes tiempo.\n",
    "    batch_size=512,\n",
    "    validation_data=(X_test_3D, y_test_cat) # Validamos con los datos de test\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Entrenamiento de TCN completado en {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "\n",
    "# --- 4. Evaluar el Modelo TCN ---\n",
    "print(\"\\n--- Reporte de ClasificaciÃ³n (Modelo TCN) ---\")\n",
    "\n",
    "# Predecir sobre X_test_3D\n",
    "y_pred_probs = model_tcn.predict(X_test_3D)\n",
    "\n",
    "# Las predicciones 'y_pred_probs' son probabilidades.\n",
    "# Necesitamos la clase con la probabilidad mÃ¡s alta (argmax)\n",
    "y_pred_tcn = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Generar el reporte\n",
    "# Comparamos 'y_test' (los nÃºmeros 0-33) con 'y_pred_tcn'\n",
    "report_tcn = classification_report(\n",
    "    y_test,\n",
    "    y_pred_tcn,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(report_tcn)"
   ],
   "id": "bfd63b82333881e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# ETAPA 8: Guardar el Modelo y Artefactos\n",
    "# ----------------------------------------------\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"--- Guardando el sistema completo ---\")\n",
    "\n",
    "# 1. Guardar el Modelo TCN (.keras es el formato moderno de TensorFlow)\n",
    "model_filename = 'modelo_tcn_iot.keras'\n",
    "model_tcn.save(model_filename)\n",
    "print(f\"âœ… Modelo guardado como: {model_filename}\")\n",
    "\n",
    "# 2. Guardar el Scaler (MinMaxScaler)\n",
    "scaler_filename = 'scaler_entrenado.joblib'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"âœ… Scaler guardado como: {scaler_filename}\")\n",
    "\n",
    "# 3. Guardar el LabelEncoder\n",
    "le_filename = 'label_encoder_entrenado.joblib'\n",
    "joblib.dump(le, le_filename)\n",
    "print(f\"âœ… LabelEncoder guardado como: {le_filename}\")\n",
    "\n",
    "print(\"\\nÂ¡Todo listo! Copia estos 3 archivos a la carpeta donde correrÃ¡s tu script final.\")"
   ],
   "id": "9452a1242867f0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------\n",
    "# Script de Inferencia (Uso futuro)\n",
    "# ----------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tcn import TCN # Â¡Necesitas importar esto para cargar el modelo!\n",
    "\n",
    "# --- 1. Cargar los artefactos ---\n",
    "print(\"Cargando cerebro de la IA...\")\n",
    "\n",
    "# Cargar Scaler y Encoder\n",
    "scaler = joblib.load('scaler_entrenado.joblib')\n",
    "le = joblib.load('label_encoder_entrenado.joblib')\n",
    "\n",
    "# Cargar Modelo TCN\n",
    "# Usamos custom_objects para que Keras entienda la capa TCN\n",
    "model = load_model('modelo_tcn_iot.keras', custom_objects={'TCN': TCN})\n",
    "\n",
    "print(\"Â¡Sistema cargado y listo para detectar ataques!\")\n",
    "\n",
    "# --- 2. Ejemplo de cÃ³mo predecir un NUEVO dato ---\n",
    "\n",
    "def predecir_trafico(datos_nuevos_df):\n",
    "    \"\"\"\n",
    "    datos_nuevos_df: Un DataFrame con las 46 columnas EXACTAS\n",
    "                     que usaste para entrenar (flow_duration, etc.)\n",
    "                     y en el MISMO ORDEN.\n",
    "    \"\"\"\n",
    "\n",
    "    # A. Preprocesamiento (Escalar)\n",
    "    # Usamos .transform, NO .fit_transform\n",
    "    X_scaled = scaler.transform(datos_nuevos_df)\n",
    "\n",
    "    # B. Reformatear a 3D para la TCN (1 muestra, 1 timestep, 46 features)\n",
    "    # Asumimos que datos_nuevos_df tiene 1 sola fila para predicciÃ³n en tiempo real\n",
    "    X_3D = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "    # C. PredicciÃ³n\n",
    "    pred_probs = model.predict(X_3D, verbose=0) # verbose=0 para que no llene la consola\n",
    "    pred_class_index = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "    # D. Decodificar (NÃºmero -> Nombre del Ataque)\n",
    "    nombre_ataque = le.inverse_transform(pred_class_index)\n",
    "\n",
    "    # E. Probabilidad (Confianza)\n",
    "    confianza = np.max(pred_probs)\n",
    "\n",
    "    return nombre_ataque[0], confianza\n",
    "\n",
    "# --- PRUEBA SIMULADA ---\n",
    "# Imaginemos que acabas de capturar un paquete y extraer sus features\n",
    "# (AquÃ­ creo datos falsos aleatorios solo para probar el cÃ³digo)\n",
    "fake_data = pd.DataFrame(np.random.rand(1, 46), columns=scaler.feature_names_in_)\n",
    "\n",
    "ataque, prob = predecir_trafico(fake_data)\n",
    "\n",
    "print(f\"\\n--- Resultado del AnÃ¡lisis ---\")\n",
    "print(f\"PredicciÃ³n: {ataque}\")\n",
    "print(f\"Confianza:  {prob:.2%}\")\n",
    "\n",
    "if ataque == \"BenignTraffic\":\n",
    "    print(\"ðŸŸ¢ TrÃ¡fico Seguro\")\n",
    "else:\n",
    "    print(\"ðŸ”´ Â¡ALERTA! Ataque Detectado\")"
   ],
   "id": "ebd8f722032ac860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FEATURES_SELECCIONADAS = [\n",
    "    # --- Tiempo y Volumen (Detecta DoS/DDoS) ---\n",
    "    'flow_duration', 'iat',\n",
    "    'rate', 'srate', 'drate',\n",
    "    'tot_sum', 'tot_size', 'avg', 'std',\n",
    "\n",
    "    # --- Flags TCP y Conteos (Detecta Escaneos y Comportamiento) ---\n",
    "    'syn_flag_number', 'rst_flag_number', 'ack_flag_number', 'fin_flag_number', 'psh_flag_number',\n",
    "    'ack_count', 'syn_count', 'fin_count', 'rst_count',\n",
    "\n",
    "    # --- Protocolos (Identidad) ---\n",
    "    'protocol_type', 'tcp', 'udp', 'icmp',\n",
    "\n",
    "    # --- AplicaciÃ³n (Contexto IoT - Mirai/Botnets) ---\n",
    "    'http', 'https', 'telnet', 'dns',\n",
    "\n",
    "    # --- La Etiqueta a Predecir ---\n",
    "    'label'\n",
    "]"
   ],
   "id": "83e202fbc2e1bfe4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
